{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGcZm13-47Nx",
        "outputId": "2129d04b-8869-44a0-80b0-8df72d9e7bd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets groq together transformers pandas numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QP28x26iqjG"
      },
      "source": [
        "Part 1: Prompt Design & Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kyAErPty4-iu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "from datetime import datetime\n",
        "from groq import Groq\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxX7X8gB5Qvt"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY = \"your-api-key\"\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "1168bf83677a4d5b8f764359811a8c7f",
            "538a79ceafcd4a4496a18eebc1000f26",
            "c6581dccf386421dae004cd21dbdc7e3",
            "1c8610d488df4fbfb054ffa6e04e503e",
            "d163ccbd5e494086bad2cbe544a9dd98",
            "3b07808cc798403a831e50d8a61592d0",
            "9dc13497cecc4be89e055448000447da",
            "1de4f2537eb04b38a38ad1df730b9969",
            "5b35f5c56f204173a0d3e6af3b9e9f36",
            "c3e5a7281c4c48d091b30b7d7d3d62aa",
            "8c75c7524f434a369c6ae5bf30803a0a",
            "30b05ed8c3a3495c8e4cd3044af3cdc4",
            "5d9fafbfe6b342368d0aaba0e416d932",
            "92f37bebbf3443a8a4f5f3412b4fb657",
            "5af8cc180e214f879f2557a339ea68f9",
            "58be00c616424206a294b0a4ed514f41",
            "a056f9488b224b77884d7810422404ed",
            "11a0992da8bb486e8eccf83f0c40b0e9",
            "6e3fc0fed26549a4ad12b0c02b1d45f3",
            "3291b98275e442b89b2a7ff5edaf8bcd",
            "2b05f60b3d8d4915b998e4aa56e7fb5a",
            "75e00f44589a41808059229caf0f13e9",
            "4bf65a1117084f149909442fe029fa49",
            "5624912c909d4a8f961fc481cfd263d5",
            "1c0bed6c5605449b9813d6b9315f0a02",
            "9ae238fa23fc453789d65c6d3d4f2921",
            "7c8844be43b747fdae53bcb17089ee88",
            "45d7a0caec6f48dd94b3cfe6706e7985",
            "0ea308f63e98481d9e5bf5cdc5b06b00",
            "8e6ed60d9d4547ef9e979724baaecf39",
            "ec0ad7c545eb409ebfe4ac70fa80bd8e",
            "d3c73718d1224be1b9a0e773088598fc",
            "36e0bbab8fd94e3f8a757b6c087ec3b2"
          ]
        },
        "id": "wLZlkfaF7OGs",
        "outputId": "5b1bc344-e831-4fb7-dcfa-fa87142cc3d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1168bf83677a4d5b8f764359811a8c7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30b05ed8c3a3495c8e4cd3044af3cdc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "openai_humaneval/test-00000-of-00001.par(…):   0%|          | 0.00/83.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bf65a1117084f149909442fe029fa49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/164 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ds = load_dataset(\"openai/openai_humaneval\")\n",
        "humaneval = ds[\"test\"].to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rJjKWMsM7dc8"
      },
      "outputs": [],
      "source": [
        "selected_indices = [0, 1, 2, 3, 4, 9, 11, 14, 25, 39]\n",
        "selected_problems = humaneval.iloc[selected_indices].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNoc-3h-7hqa",
        "outputId": "16032461-fdb1-4b57-ab25-385aad188f1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Problem IDs: ['HumanEval/0', 'HumanEval/1', 'HumanEval/2', 'HumanEval/3', 'HumanEval/4', 'HumanEval/9', 'HumanEval/11', 'HumanEval/14', 'HumanEval/25', 'HumanEval/39']\n"
          ]
        }
      ],
      "source": [
        "print(\"Problem IDs:\", selected_problems['task_id'].tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Jv6MqYcH7mre"
      },
      "outputs": [],
      "source": [
        "class PromptStrategies:\n",
        "    \"\"\"Six different prompting strategies as required\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def chain_of_thought(prompt):\n",
        "        return f\"\"\"Let's think about this step by step.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "First, I'll understand the requirements, then write the solution.\n",
        "Let me work through this methodically:\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def stepwise_cot(prompt):\n",
        "        return f\"\"\"I'll solve this problem following these explicit steps:\n",
        "Step 1: Parse the function signature and understand inputs/outputs\n",
        "Step 2: Analyze the provided examples\n",
        "Step 3: Identify edge cases to handle\n",
        "Step 4: Write the implementation\n",
        "\n",
        "{prompt}\n",
        "\n",
        "Following each step:\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def self_planning(prompt):\n",
        "        return f\"\"\"I need to create a plan before implementing this.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "My implementation plan:\n",
        "1. Data structures needed:\n",
        "2. Algorithm approach:\n",
        "3. Edge cases to handle:\n",
        "\n",
        "Now implementing based on this plan:\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def self_debugging(prompt):\n",
        "        return f\"\"\"I'll write this solution and then check it for bugs.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "Initial implementation (then I'll review for bugs):\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def self_edit(prompt):\n",
        "        return f\"\"\"I'll implement this focusing on clean, efficient code.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "Here's my optimized solution:\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def self_repair(prompt):\n",
        "        return f\"\"\"I'll write a robust solution with proper error handling.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "Robust implementation with error handling:\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9T2nNNcX7xSh"
      },
      "outputs": [],
      "source": [
        "def call_openai(prompt):\n",
        "    \"\"\"Call openai model (Model Family 1: Mixture of Experts)\"\"\"\n",
        "    try:\n",
        "        completion = groq_client.chat.completions.create(\n",
        "            model=\"openai/gpt-oss-20b\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert Python programmer. Write clean, correct code.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=800,\n",
        "            top_p=1,\n",
        "            stream=False\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"openai error: {e}\")\n",
        "        time.sleep(5)\n",
        "        return \"\"\n",
        "\n",
        "def call_llama(prompt):\n",
        "    \"\"\"Call Llama model (Model Family 2: Meta's LLaMA)\"\"\"\n",
        "    try:\n",
        "        completion = groq_client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert Python programmer. Write clean, correct code.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=800,\n",
        "            top_p=1,\n",
        "            stream=False\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Llama error: {e}\")\n",
        "        time.sleep(5)\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w2qB5wlm719H"
      },
      "outputs": [],
      "source": [
        "def extract_code(text):\n",
        "    \"\"\"Extract Python code from model response\"\"\"\n",
        "\n",
        "    if \"```python\" in text:\n",
        "        pattern = r'```python\\n(.*?)```'\n",
        "        matches = re.findall(pattern, text, re.DOTALL)\n",
        "        if matches:\n",
        "            return matches[-1].strip()\n",
        "\n",
        "    if \"```\" in text:\n",
        "        pattern = r'```\\n(.*?)```'\n",
        "        matches = re.findall(pattern, text, re.DOTALL)\n",
        "        if matches:\n",
        "            return matches[-1].strip()\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    code_lines = []\n",
        "    in_func = False\n",
        "    base_indent = 0\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip().startswith('def '):\n",
        "            in_func = True\n",
        "            base_indent = len(line) - len(line.lstrip())\n",
        "            code_lines = [line]\n",
        "        elif in_func:\n",
        "            if line.strip() == '':\n",
        "                code_lines.append(line)\n",
        "            elif len(line) - len(line.lstrip()) > base_indent:\n",
        "                code_lines.append(line)\n",
        "            elif line.strip() and not line[0].isspace():\n",
        "                break\n",
        "            else:\n",
        "                code_lines.append(line)\n",
        "\n",
        "    if code_lines:\n",
        "        return '\\n'.join(code_lines)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def test_solution(code, test_code, entry_point):\n",
        "    \"\"\"Test generated code against test cases\"\"\"\n",
        "    try:\n",
        "        namespace = {}\n",
        "\n",
        "        exec(code, namespace)\n",
        "\n",
        "        exec(test_code, namespace)\n",
        "\n",
        "        if 'check' in namespace and entry_point in namespace:\n",
        "            try:\n",
        "                namespace['check'](namespace[entry_point])\n",
        "                return True, \"All tests passed\"\n",
        "            except AssertionError as e:\n",
        "                return False, f\"Test failed: {str(e)}\"\n",
        "            except Exception as e:\n",
        "                return False, f\"Runtime error: {str(e)}\"\n",
        "        else:\n",
        "            return False, \"Function not found in generated code\"\n",
        "\n",
        "    except SyntaxError as e:\n",
        "        return False, f\"Syntax error: {str(e)}\"\n",
        "    except Exception as e:\n",
        "        return False, f\"Execution error: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJJ4nfyN8CZG",
        "outputId": "b6976fe9-1d9c-445a-8f3f-f5d5726ee851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Part 1 Experiment...\n",
            "\n",
            "============================================================\n",
            "PART 1: Prompt Design & Code Generation\n",
            "Testing 6 strategies × 2 model families × 10 problems\n",
            "============================================================\n",
            "\n",
            "Problem 1/10: HumanEval/0\n",
            "  [1/120] CoT + openai/gpt-oss-20b: Fail\n",
            "  [2/120] CoT + Llama3-70B: Pass\n",
            "  [3/120] SCoT + openai/gpt-oss-20b: Fail\n",
            "  [4/120] SCoT + Llama3-70B: Fail\n",
            "  [5/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [6/120] Self-Plan + Llama3-70B: False\n",
            "True\n",
            "Pass\n",
            "  [7/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [8/120] Self-Debug + Llama3-70B: False\n",
            "True\n",
            "Pass\n",
            "  [9/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [10/120] Self-Edit + Llama3-70B: Pass\n",
            "  [11/120] Self-Repair + openai/gpt-oss-20b: ↻Fail\n",
            "  [12/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 2/10: HumanEval/1\n",
            "  [13/120] CoT + openai/gpt-oss-20b: Pass\n",
            "  [14/120] CoT + Llama3-70B: Fail\n",
            "  [15/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [16/120] SCoT + Llama3-70B: Pass\n",
            "  [17/120] Self-Plan + openai/gpt-oss-20b: ↻Fail\n",
            "  [18/120] Self-Plan + Llama3-70B: ['()', '(())', '(()())']\n",
            "Pass\n",
            "  [19/120] Self-Debug + openai/gpt-oss-20b: Fail\n",
            "  [20/120] Self-Debug + Llama3-70B: ['()', '(())', '(()())']\n",
            "Pass\n",
            "  [21/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [22/120] Self-Edit + Llama3-70B: ['()', '(())', '(()())']\n",
            "Pass\n",
            "  [23/120] Self-Repair + openai/gpt-oss-20b: Fail\n",
            "  [24/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 3/10: HumanEval/2\n",
            "  [25/120] CoT + openai/gpt-oss-20b: ↻Pass\n",
            "  [26/120] CoT + Llama3-70B: Pass\n",
            "  [27/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [28/120] SCoT + Llama3-70B: Fail\n",
            "  [29/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [30/120] Self-Plan + Llama3-70B: 0.5\n",
            "Pass\n",
            "  [31/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [32/120] Self-Debug + Llama3-70B: Pass\n",
            "  [33/120] Self-Edit + openai/gpt-oss-20b: Fail\n",
            "  [34/120] Self-Edit + Llama3-70B: Pass\n",
            "  [35/120] Self-Repair + openai/gpt-oss-20b: ↻Pass\n",
            "  [36/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 4/10: HumanEval/3\n",
            "  [37/120] CoT + openai/gpt-oss-20b: Pass\n",
            "  [38/120] CoT + Llama3-70B: False\n",
            "True\n",
            "Pass\n",
            "  [39/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [40/120] SCoT + Llama3-70B: Fail\n",
            "  [41/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [42/120] Self-Plan + Llama3-70B: False\n",
            "True\n",
            "False\n",
            "False\n",
            "True\n",
            "Pass\n",
            "  [43/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [44/120] Self-Debug + Llama3-70B: False\n",
            "True\n",
            "Pass\n",
            "  [45/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [46/120] Self-Edit + Llama3-70B: False\n",
            "True\n",
            "Pass\n",
            "  [47/120] Self-Repair + openai/gpt-oss-20b: Pass\n",
            "  [48/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 5/10: HumanEval/4\n",
            "  [49/120] CoT + openai/gpt-oss-20b: ↻Pass\n",
            "  [50/120] CoT + Llama3-70B: 1.0\n",
            "Pass\n",
            "  [51/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [52/120] SCoT + Llama3-70B: Pass\n",
            "  [53/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [54/120] Self-Plan + Llama3-70B: 1.0\n",
            "Pass\n",
            "  [55/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [56/120] Self-Debug + Llama3-70B: 1.0\n",
            "Pass\n",
            "  [57/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [58/120] Self-Edit + Llama3-70B: 1.0\n",
            "Pass\n",
            "  [59/120] Self-Repair + openai/gpt-oss-20b: Pass\n",
            "  [60/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 6/10: HumanEval/9\n",
            "  [61/120] CoT + openai/gpt-oss-20b: Pass\n",
            "  [62/120] CoT + Llama3-70B: [1, 2, 3, 3, 3, 4, 4]\n",
            "Pass\n",
            "  [63/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [64/120] SCoT + Llama3-70B: [1, 2, 3, 3, 3, 4, 4]\n",
            "Pass\n",
            "  [65/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [66/120] Self-Plan + Llama3-70B: [1, 2, 3, 3, 3, 4, 4]\n",
            "Pass\n",
            "  [67/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [68/120] Self-Debug + Llama3-70B: [1, 2, 3, 3, 3, 4, 4]\n",
            "Pass\n",
            "  [69/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [70/120] Self-Edit + Llama3-70B: [1, 2, 3, 3, 3, 4, 4]\n",
            "Pass\n",
            "  [71/120] Self-Repair + openai/gpt-oss-20b: Fail\n",
            "  [72/120] Self-Repair + Llama3-70B: Fail\n",
            "\n",
            "Problem 7/10: HumanEval/11\n",
            "  [73/120] CoT + openai/gpt-oss-20b: Pass\n",
            "  [74/120] CoT + Llama3-70B: 100\n",
            "Pass\n",
            "  [75/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [76/120] SCoT + Llama3-70B: 100\n",
            "Pass\n",
            "  [77/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [78/120] Self-Plan + Llama3-70B: 100\n",
            "Pass\n",
            "  [79/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [80/120] Self-Debug + Llama3-70B: 100\n",
            "Pass\n",
            "  [81/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [82/120] Self-Edit + Llama3-70B: 100\n",
            "Pass\n",
            "  [83/120] Self-Repair + openai/gpt-oss-20b: Fail\n",
            "  [84/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 8/10: HumanEval/14\n",
            "  [85/120] CoT + openai/gpt-oss-20b: Pass\n",
            "  [86/120] CoT + Llama3-70B: ['a', 'ab', 'abc']\n",
            "Pass\n",
            "  [87/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [88/120] SCoT + Llama3-70B: ['a', 'ab', 'abc']\n",
            "[]\n",
            "['a']\n",
            "Pass\n",
            "  [89/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [90/120] Self-Plan + Llama3-70B: ['a', 'ab', 'abc']\n",
            "[]\n",
            "['a']\n",
            "Pass\n",
            "  [91/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [92/120] Self-Debug + Llama3-70B: ['a', 'ab', 'abc']\n",
            "Pass\n",
            "  [93/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [94/120] Self-Edit + Llama3-70B: Pass\n",
            "  [95/120] Self-Repair + openai/gpt-oss-20b: ↻↻Pass\n",
            "  [96/120] Self-Repair + Llama3-70B: Fail\n",
            "\n",
            "Problem 9/10: HumanEval/25\n",
            "  [97/120] CoT + openai/gpt-oss-20b: Pass\n",
            "  [98/120] CoT + Llama3-70B: [2, 2, 2]\n",
            "[5, 5]\n",
            "[2, 5, 7]\n",
            "Pass\n",
            "  [99/120] SCoT + openai/gpt-oss-20b: Fail\n",
            "  [100/120] SCoT + Llama3-70B: Fail\n",
            "  [101/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [102/120] Self-Plan + Llama3-70B: [2, 2, 2]\n",
            "[5, 5]\n",
            "[2, 5, 7]\n",
            "Pass\n",
            "  [103/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [104/120] Self-Debug + Llama3-70B: Fail\n",
            "  [105/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [106/120] Self-Edit + Llama3-70B: [2, 2, 2]\n",
            "[5, 5]\n",
            "[2, 5, 7]\n",
            "Pass\n",
            "  [107/120] Self-Repair + openai/gpt-oss-20b: Fail\n",
            "  [108/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 10/10: HumanEval/39\n",
            "  [109/120] CoT + openai/gpt-oss-20b: Fail\n",
            "  [110/120] CoT + Llama3-70B: Pass\n",
            "  [111/120] SCoT + openai/gpt-oss-20b: Fail\n",
            "  [112/120] SCoT + Llama3-70B: 2\n",
            "3\n",
            "5\n",
            "13\n",
            "89\n",
            "Pass\n",
            "  [113/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [114/120] Self-Plan + Llama3-70B: 2\n",
            "3\n",
            "5\n",
            "13\n",
            "89\n",
            "Pass\n",
            "  [115/120] Self-Debug + openai/gpt-oss-20b: Fail\n",
            "  [116/120] Self-Debug + Llama3-70B: 2\n",
            "3\n",
            "5\n",
            "13\n",
            "89\n",
            "Pass\n",
            "  [117/120] Self-Edit + openai/gpt-oss-20b: Fail\n",
            "  [118/120] Self-Edit + Llama3-70B: 2\n",
            "3\n",
            "5\n",
            "13\n",
            "89\n",
            "Pass\n",
            "  [119/120] Self-Repair + openai/gpt-oss-20b: ↻↻Fail\n",
            "  [120/120] Self-Repair + Llama3-70B: Pass\n"
          ]
        }
      ],
      "source": [
        "def run_part1_experiment():\n",
        "    \"\"\"Part 1: Test prompting strategies (8 points)\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PART 1: Prompt Design & Code Generation\")\n",
        "    print(\"Testing 6 strategies × 2 model families × 10 problems\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    strategies = {\n",
        "        \"CoT\": PromptStrategies.chain_of_thought,\n",
        "        \"SCoT\": PromptStrategies.stepwise_cot,\n",
        "        \"Self-Plan\": PromptStrategies.self_planning,\n",
        "        \"Self-Debug\": PromptStrategies.self_debugging,\n",
        "        \"Self-Edit\": PromptStrategies.self_edit,\n",
        "        \"Self-Repair\": PromptStrategies.self_repair\n",
        "    }\n",
        "\n",
        "    models = {\n",
        "        \"openai/gpt-oss-20b\": call_openai,\n",
        "        \"Llama3-70B\": call_llama\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    total_tests = len(selected_problems) * len(strategies) * len(models)\n",
        "    current_test = 0\n",
        "\n",
        "    for prob_idx, problem in selected_problems.iterrows():\n",
        "        print(f\"\\nProblem {prob_idx+1}/10: {problem['task_id']}\")\n",
        "\n",
        "        for strat_name, strat_func in strategies.items():\n",
        "            for model_name, model_func in models.items():\n",
        "                current_test += 1\n",
        "                print(f\"  [{current_test}/{total_tests}] {strat_name} + {model_name}: \", end=\"\", flush=True)\n",
        "\n",
        "                prompt = strat_func(problem['prompt'])\n",
        "\n",
        "                response = \"\"\n",
        "                for attempt in range(3):\n",
        "                    response = model_func(prompt)\n",
        "                    if response:\n",
        "                        break\n",
        "                    print(\"↻\", end=\"\", flush=True)\n",
        "                    time.sleep(3)\n",
        "\n",
        "                code = extract_code(response)\n",
        "                success, error = test_solution(code, problem['test'], problem['entry_point'])\n",
        "\n",
        "                results.append({\n",
        "                    'problem_id': problem['task_id'],\n",
        "                    'problem_idx': prob_idx,\n",
        "                    'strategy': strat_name,\n",
        "                    'model': model_name,\n",
        "                    'success': success,\n",
        "                    'error': error if not success else None,\n",
        "                    'prompt_used': prompt[:300] + \"...\" if len(prompt) > 300 else prompt,\n",
        "                    'response': response[:500] + \"...\" if len(response) > 500 else response,\n",
        "                    'generated_code': code\n",
        "                })\n",
        "\n",
        "                print(\"Pass\" if success else \"Fail\")\n",
        "\n",
        "                time.sleep(2.5)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"Starting Part 1 Experiment...\")\n",
        "part1_results = run_part1_experiment()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16ju75gW8IAr",
        "outputId": "61cc4a75-3d8d-4389-8ca2-368c8e85961a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PART 1: RESULTS ANALYSIS\n",
            "============================================================\n",
            "\n",
            "Overall Success Rate: 72.5%\n",
            "   Successful: 87/120\n",
            "\n",
            "Success Rate by Strategy:\n",
            "   Self-Edit: 90.0% (18/20 passed)\n",
            "   CoT: 85.0% (17/20 passed)\n",
            "   Self-Debug: 85.0% (17/20 passed)\n",
            "   SCoT: 65.0% (13/20 passed)\n",
            "   Self-Repair: 60.0% (12/20 passed)\n",
            "   Self-Plan: 50.0% (10/20 passed)\n",
            "\n",
            "Success Rate by Model Family:\n",
            "   Llama3-70B: 86.7% (52/60 passed)\n",
            "   openai/gpt-oss-20b: 58.3% (35/60 passed)\n",
            "\n",
            "Strategy × Model Success Rate (%):\n",
            "model        Llama3-70B  openai/gpt-oss-20b  Average\n",
            "strategy                                            \n",
            "CoT                90.0                80.0     85.0\n",
            "SCoT               60.0                70.0     65.0\n",
            "Self-Debug         90.0                80.0     85.0\n",
            "Self-Edit         100.0                80.0     90.0\n",
            "Self-Plan         100.0                 0.0     50.0\n",
            "Self-Repair        80.0                40.0     60.0\n",
            "\n",
            "Top 3 Strategy-Model Combinations:\n",
            "   Self-Plan + Llama3-70B: 100.0%\n",
            "   Self-Edit + Llama3-70B: 100.0%\n",
            "   CoT + Llama3-70B: 90.0%\n",
            "\n",
            "Results saved to part1_results.csv\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PART 1: RESULTS ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "overall_success = part1_results['success'].mean()\n",
        "print(f\"\\nOverall Success Rate: {overall_success*100:.1f}%\")\n",
        "print(f\"   Successful: {part1_results['success'].sum()}/{len(part1_results)}\")\n",
        "\n",
        "print(\"\\nSuccess Rate by Strategy:\")\n",
        "strategy_stats = part1_results.groupby('strategy')['success'].agg(['mean', 'sum', 'count'])\n",
        "strategy_stats['percentage'] = strategy_stats['mean'] * 100\n",
        "strategy_stats = strategy_stats.sort_values('percentage', ascending=False)\n",
        "for idx, row in strategy_stats.iterrows():\n",
        "    print(f\"   {idx}: {row['percentage']:.1f}% ({int(row['sum'])}/{int(row['count'])} passed)\")\n",
        "\n",
        "print(\"\\nSuccess Rate by Model Family:\")\n",
        "model_stats = part1_results.groupby('model')['success'].agg(['mean', 'sum', 'count'])\n",
        "model_stats['percentage'] = model_stats['mean'] * 100\n",
        "for idx, row in model_stats.iterrows():\n",
        "    print(f\"   {idx}: {row['percentage']:.1f}% ({int(row['sum'])}/{int(row['count'])} passed)\")\n",
        "\n",
        "print(\"\\nStrategy × Model Success Rate (%):\")\n",
        "cross_tab = part1_results.pivot_table(\n",
        "    values='success',\n",
        "    index='strategy',\n",
        "    columns='model',\n",
        "    aggfunc='mean'\n",
        ") * 100\n",
        "cross_tab['Average'] = cross_tab.mean(axis=1)\n",
        "print(cross_tab.round(1))\n",
        "\n",
        "print(\"\\nTop 3 Strategy-Model Combinations:\")\n",
        "combo_stats = part1_results.groupby(['strategy', 'model'])['success'].mean().sort_values(ascending=False).head(3)\n",
        "for (strat, model), rate in combo_stats.items():\n",
        "    print(f\"   {strat} + {model}: {rate*100:.1f}%\")\n",
        "\n",
        "part1_results.to_csv('part1_results.csv', index=False)\n",
        "print(\"\\nResults saved to part1_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFg0kqnxi7wd"
      },
      "source": [
        "Part 2: Debugging & Iterative Improvement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnwzKU8l8wEk",
        "outputId": "b7ebceb8-188b-48c5-d507-5406d05f6c4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PART 2: Debugging & Iterative Improvement\n",
            "============================================================\n",
            "\n",
            "Debugging Case 1:\n",
            "   Problem: HumanEval/1\n",
            "   Original: CoT + Llama3-70B\n",
            "   Error: Execution error: name 'List' is not defined...\n",
            "   Attempting fix with openai... Fixed!\n",
            "   Analysis: The model successfully identified and fixed the issue.\n",
            "\n",
            "Debugging Case 2:\n",
            "   Problem: HumanEval/0\n",
            "   Original: CoT + openai/gpt-oss-20b\n",
            "   Error: Syntax error: invalid syntax (<string>, line 42)...\n",
            "   Attempting fix with openai... Fixed!\n",
            "   Analysis: The model successfully identified and fixed the issue.\n"
          ]
        }
      ],
      "source": [
        "def run_part2_debugging():\n",
        "    \"\"\"Part 2: Debug failed cases\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PART 2: Debugging & Iterative Improvement\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    failed = part1_results[~part1_results['success']]\n",
        "\n",
        "    if len(failed) < 2:\n",
        "        print(\"Less than 2 failed cases available!\")\n",
        "        failed = part1_results.head(2)\n",
        "    else:\n",
        "        failed = failed.groupby(['strategy', 'model']).first().head(2).reset_index()\n",
        "\n",
        "    debug_results = []\n",
        "\n",
        "    for idx in range(min(2, len(failed))):\n",
        "        fail = failed.iloc[idx]\n",
        "        print(f\"\\nDebugging Case {idx+1}:\")\n",
        "        print(f\"   Problem: {fail['problem_id']}\")\n",
        "        print(f\"   Original: {fail['strategy']} + {fail['model']}\")\n",
        "        print(f\"   Error: {fail['error'][:100]}...\")\n",
        "\n",
        "        prob = selected_problems[selected_problems['task_id'] == fail['problem_id']].iloc[0]\n",
        "\n",
        "        debug_prompt = f\"\"\"You are debugging Python code. Fix the following error.\n",
        "\n",
        "PROBLEM DESCRIPTION:\n",
        "{prob['prompt']}\n",
        "\n",
        "FAILED CODE:\n",
        "```python\n",
        "{fail['generated_code']}\n",
        "```\n",
        "\n",
        "ERROR MESSAGE:\n",
        "{fail['error']}\n",
        "\n",
        "Please provide a CORRECTED version that will pass all test cases.\n",
        "Think about what went wrong and fix it.\n",
        "\n",
        "CORRECTED CODE:\"\"\"\n",
        "\n",
        "        print(\"   Attempting fix with openai...\", end=\"\", flush=True)\n",
        "\n",
        "        fixed_response = call_openai(debug_prompt)\n",
        "        fixed_code = extract_code(fixed_response)\n",
        "\n",
        "        success, error = test_solution(fixed_code, prob['test'], prob['entry_point'])\n",
        "\n",
        "        debug_results.append({\n",
        "            'problem_id': fail['problem_id'],\n",
        "            'original_strategy': fail['strategy'],\n",
        "            'original_model': fail['model'],\n",
        "            'original_error': fail['error'],\n",
        "            'debug_prompt': debug_prompt[:500] + \"...\",\n",
        "            'fixed_code': fixed_code,\n",
        "            'debug_success': success,\n",
        "            'debug_error': error if not success else None,\n",
        "            'improvement': \"Fixed\" if success else \"Still failing\"\n",
        "        })\n",
        "\n",
        "        print(f\" {'Fixed!' if success else 'Still has issues'}\")\n",
        "\n",
        "        if success:\n",
        "            print(f\"   Analysis: The model successfully identified and fixed the issue.\")\n",
        "        else:\n",
        "            print(f\"   Analysis: Debugging attempt improved but didn't fully resolve the issue.\")\n",
        "            print(f\"   Remaining error: {error[:100]}...\")\n",
        "\n",
        "        time.sleep(3)\n",
        "\n",
        "    return debug_results\n",
        "\n",
        "part2_results = run_part2_debugging()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRv_n28f806u",
        "outputId": "f71da12b-1e67-4d34-f9f8-3c4f5d9cdf6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Debugging Summary:\n",
            "   Success rate: 100% (2/2)\n",
            "\n",
            "   HumanEval/1:\n",
            "   - Original: CoT + Llama3-70B\n",
            "   - Result: Fixed\n",
            "\n",
            "   HumanEval/0:\n",
            "   - Original: CoT + openai/gpt-oss-20b\n",
            "   - Result: Fixed\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nDebugging Summary:\")\n",
        "if part2_results:\n",
        "    debug_success_rate = sum(r['debug_success'] for r in part2_results) / len(part2_results)\n",
        "    print(f\"   Success rate: {debug_success_rate*100:.0f}% ({sum(r['debug_success'] for r in part2_results)}/{len(part2_results)})\")\n",
        "\n",
        "    for r in part2_results:\n",
        "        print(f\"\\n   {r['problem_id']}:\")\n",
        "        print(f\"   - Original: {r['original_strategy']} + {r['original_model']}\")\n",
        "        print(f\"   - Result: {r['improvement']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UykqPLVFjC_X"
      },
      "source": [
        "Part 3: Innovation - Test-Driven Incremental Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB9G2Kfd89XP",
        "outputId": "c4bddd7b-9d14-4478-8389-41c4c4353a46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PART 3: Innovation - Test-Driven Incremental Generation\n",
            "============================================================\n",
            "\n",
            "Innovation Strategy: Test-Driven Incremental Generation\n",
            "   - Shows concrete test examples upfront\n",
            "   - Iteratively refines based on test failures\n",
            "   - Maximum 3 iterations per problem\n",
            "\n",
            "Testing innovative strategy on selected problems:\n",
            "\n",
            "Problem: HumanEval/0\n",
            "   openai/gpt-oss-20b (openai family):\n",
            "      Iteration 1: Pass Success!\n",
            "   Llama3-70B (LLaMA family):\n",
            "      Iteration 1: Pass Success!\n",
            "Problem: HumanEval/1\n",
            "   openai/gpt-oss-20b (openai family):\n",
            "      Iteration 1: Fail      Iteration 2: Fail      Iteration 3: Fail Max iterations reached\n",
            "   Llama3-70B (LLaMA family):\n",
            "      Iteration 1: Pass Success!\n",
            "\n",
            "Comparative Analysis:\n",
            "\n",
            "Performance Comparison:\n",
            "      Baseline strategies (avg): 62.5%\n",
            "      TDIG Innovation strategy: 75.0%\n",
            "      Relative improvement: +12.5%\n",
            "\n",
            "Efficiency Analysis:\n",
            "      Average iterations needed: 1.5\n",
            "      Success on first try: 2/4\n"
          ]
        }
      ],
      "source": [
        "def run_part3_innovation():\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PART 3: Innovation - Test-Driven Incremental Generation\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nInnovation Strategy: Test-Driven Incremental Generation\")\n",
        "    print(\"   - Shows concrete test examples upfront\")\n",
        "    print(\"   - Iteratively refines based on test failures\")\n",
        "    print(\"   - Maximum 3 iterations per problem\")\n",
        "\n",
        "    def test_driven_generation(problem):\n",
        "        test_lines = []\n",
        "        for line in problem['test'].split('\\n'):\n",
        "            if 'assert candidate' in line:\n",
        "                test_lines.append(line.strip())\n",
        "                if len(test_lines) >= 2:\n",
        "                    break\n",
        "\n",
        "        prompt = f\"\"\"Look at these specific test cases that your function must pass:\n",
        "\n",
        "{chr(10).join(test_lines)}\n",
        "\n",
        "Now implement this function to pass these tests:\n",
        "\n",
        "{problem['prompt']}\n",
        "\n",
        "Focus on making sure your implementation handles these exact test cases correctly.\n",
        "Write the complete function:\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def iterative_refinement(problem, model_func, model_name, max_iterations=3):\n",
        "\n",
        "        iteration_results = []\n",
        "        current_prompt = test_driven_generation(problem)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            print(f\"      Iteration {iteration+1}: \", end=\"\", flush=True)\n",
        "\n",
        "            response = model_func(current_prompt)\n",
        "            code = extract_code(response)\n",
        "\n",
        "            success, error = test_solution(code, problem['test'], problem['entry_point'])\n",
        "\n",
        "            iteration_results.append({\n",
        "                'iteration': iteration + 1,\n",
        "                'success': success,\n",
        "                'error': error if not success else None\n",
        "            })\n",
        "\n",
        "            print(\"Pass\" if success else \"Fail\", end=\"\", flush=True)\n",
        "\n",
        "            if success:\n",
        "                print(f\" Success!\")\n",
        "                return {\n",
        "                    'success': True,\n",
        "                    'iterations': iteration + 1,\n",
        "                    'final_code': code,\n",
        "                    'iteration_details': iteration_results\n",
        "                }\n",
        "\n",
        "            current_prompt = f\"\"\"The previous code failed with this error:\n",
        "{error}\n",
        "\n",
        "Failed code:\n",
        "```python\n",
        "{code}\n",
        "```\n",
        "\n",
        "Fix this specific error and make the code pass all tests.\n",
        "Remember the tests that must pass:\n",
        "{chr(10).join([line.strip() for line in problem['test'].split(chr(10)) if 'assert candidate' in line][:2])}\n",
        "\n",
        "Corrected code:\"\"\"\n",
        "\n",
        "            time.sleep(3)\n",
        "\n",
        "        print(f\" Max iterations reached\")\n",
        "        return {\n",
        "            'success': False,\n",
        "            'iterations': max_iterations,\n",
        "            'final_code': code,\n",
        "            'iteration_details': iteration_results\n",
        "        }\n",
        "\n",
        "    test_problems = selected_problems.head(2)\n",
        "    innovation_results = []\n",
        "\n",
        "    print(\"\\nTesting innovative strategy on selected problems:\\n\")\n",
        "\n",
        "    for idx, prob in test_problems.iterrows():\n",
        "        print(f\"Problem: {prob['task_id']}\")\n",
        "\n",
        "        print(f\"   openai/gpt-oss-20b (openai family):\")\n",
        "        mixtral_result = iterative_refinement(prob, call_openai, \"openai\")\n",
        "\n",
        "        print(f\"   Llama3-70B (LLaMA family):\")\n",
        "        llama_result = iterative_refinement(prob, call_llama, \"Llama\")\n",
        "\n",
        "        innovation_results.append({\n",
        "            'problem': prob['task_id'],\n",
        "            'mixtral_success': mixtral_result['success'],\n",
        "            'mixtral_iterations': mixtral_result['iterations'],\n",
        "            'llama_success': llama_result['success'],\n",
        "            'llama_iterations': llama_result['iterations']\n",
        "        })\n",
        "\n",
        "        time.sleep(3)\n",
        "\n",
        "    print(\"\\nComparative Analysis:\")\n",
        "\n",
        "    baseline_subset = part1_results[part1_results['problem_id'].isin(test_problems['task_id'])]\n",
        "    baseline_rate = baseline_subset['success'].mean()\n",
        "\n",
        "    total_innovation_tests = len(innovation_results) * 2\n",
        "    innovation_successes = sum(r['mixtral_success'] + r['llama_success'] for r in innovation_results)\n",
        "    innovation_rate = innovation_successes / total_innovation_tests if total_innovation_tests > 0 else 0\n",
        "\n",
        "    print(f\"\\nPerformance Comparison:\")\n",
        "    print(f\"      Baseline strategies (avg): {baseline_rate*100:.1f}%\")\n",
        "    print(f\"      TDIG Innovation strategy: {innovation_rate*100:.1f}%\")\n",
        "    improvement = (innovation_rate - baseline_rate) * 100\n",
        "    print(f\"      Relative improvement: {'+' if improvement >= 0 else ''}{improvement:.1f}%\")\n",
        "\n",
        "    print(f\"\\nEfficiency Analysis:\")\n",
        "    avg_iterations = sum(r['mixtral_iterations'] + r['llama_iterations'] for r in innovation_results) / (len(innovation_results) * 2)\n",
        "    print(f\"      Average iterations needed: {avg_iterations:.1f}\")\n",
        "    print(f\"      Success on first try: {sum(1 for r in innovation_results if r['mixtral_iterations'] == 1 or r['llama_iterations'] == 1)}/{total_innovation_tests}\")\n",
        "\n",
        "    return innovation_results\n",
        "\n",
        "part3_results = run_part3_innovation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPYNBxab9DG8",
        "outputId": "d666e44a-cb35-4168-ff08-15d509b2b211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Final Summary\n",
            "============================================================\n",
            "\n",
            "Key Results:\n",
            "   • Best Strategy: Self-Edit (90.0%)\n",
            "   • Best Model: Llama3-70B (86.7%)\n",
            "   • Overall Success Rate: 72.5%\n",
            "   • Debugging Success: 100%\n",
            "   • Innovation Strategy: 75%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Final Summary\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(\"\\nKey Results:\")\n",
        "best_strategy = part1_results.groupby('strategy')['success'].mean().idxmax()\n",
        "best_model = part1_results.groupby('model')['success'].mean().idxmax()\n",
        "overall_rate = part1_results['success'].mean()\n",
        "\n",
        "print(f\"   • Best Strategy: {best_strategy} ({part1_results[part1_results['strategy']==best_strategy]['success'].mean()*100:.1f}%)\")\n",
        "print(f\"   • Best Model: {best_model} ({part1_results[part1_results['model']==best_model]['success'].mean()*100:.1f}%)\")\n",
        "print(f\"   • Overall Success Rate: {overall_rate*100:.1f}%\")\n",
        "\n",
        "if part2_results:\n",
        "    debug_rate = sum(r['debug_success'] for r in part2_results) / len(part2_results)\n",
        "    print(f\"   • Debugging Success: {debug_rate*100:.0f}%\")\n",
        "\n",
        "if part3_results:\n",
        "    innovation_success = sum(r['mixtral_success'] + r['llama_success'] for r in part3_results) / (len(part3_results) * 2)\n",
        "    print(f\"   • Innovation Strategy: {innovation_success*100:.0f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um8pdxo05fQG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ea308f63e98481d9e5bf5cdc5b06b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1168bf83677a4d5b8f764359811a8c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_538a79ceafcd4a4496a18eebc1000f26",
              "IPY_MODEL_c6581dccf386421dae004cd21dbdc7e3",
              "IPY_MODEL_1c8610d488df4fbfb054ffa6e04e503e"
            ],
            "layout": "IPY_MODEL_d163ccbd5e494086bad2cbe544a9dd98"
          }
        },
        "11a0992da8bb486e8eccf83f0c40b0e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c0bed6c5605449b9813d6b9315f0a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e6ed60d9d4547ef9e979724baaecf39",
            "max": 164,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec0ad7c545eb409ebfe4ac70fa80bd8e",
            "value": 164
          }
        },
        "1c8610d488df4fbfb054ffa6e04e503e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3e5a7281c4c48d091b30b7d7d3d62aa",
            "placeholder": "​",
            "style": "IPY_MODEL_8c75c7524f434a369c6ae5bf30803a0a",
            "value": " 6.52k/? [00:00&lt;00:00, 480kB/s]"
          }
        },
        "1de4f2537eb04b38a38ad1df730b9969": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2b05f60b3d8d4915b998e4aa56e7fb5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30b05ed8c3a3495c8e4cd3044af3cdc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d9fafbfe6b342368d0aaba0e416d932",
              "IPY_MODEL_92f37bebbf3443a8a4f5f3412b4fb657",
              "IPY_MODEL_5af8cc180e214f879f2557a339ea68f9"
            ],
            "layout": "IPY_MODEL_58be00c616424206a294b0a4ed514f41"
          }
        },
        "3291b98275e442b89b2a7ff5edaf8bcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36e0bbab8fd94e3f8a757b6c087ec3b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b07808cc798403a831e50d8a61592d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45d7a0caec6f48dd94b3cfe6706e7985": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bf65a1117084f149909442fe029fa49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5624912c909d4a8f961fc481cfd263d5",
              "IPY_MODEL_1c0bed6c5605449b9813d6b9315f0a02",
              "IPY_MODEL_9ae238fa23fc453789d65c6d3d4f2921"
            ],
            "layout": "IPY_MODEL_7c8844be43b747fdae53bcb17089ee88"
          }
        },
        "538a79ceafcd4a4496a18eebc1000f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b07808cc798403a831e50d8a61592d0",
            "placeholder": "​",
            "style": "IPY_MODEL_9dc13497cecc4be89e055448000447da",
            "value": "README.md: "
          }
        },
        "5624912c909d4a8f961fc481cfd263d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45d7a0caec6f48dd94b3cfe6706e7985",
            "placeholder": "​",
            "style": "IPY_MODEL_0ea308f63e98481d9e5bf5cdc5b06b00",
            "value": "Generating test split: 100%"
          }
        },
        "58be00c616424206a294b0a4ed514f41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5af8cc180e214f879f2557a339ea68f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b05f60b3d8d4915b998e4aa56e7fb5a",
            "placeholder": "​",
            "style": "IPY_MODEL_75e00f44589a41808059229caf0f13e9",
            "value": " 83.9k/83.9k [00:00&lt;00:00, 128kB/s]"
          }
        },
        "5b35f5c56f204173a0d3e6af3b9e9f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d9fafbfe6b342368d0aaba0e416d932": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a056f9488b224b77884d7810422404ed",
            "placeholder": "​",
            "style": "IPY_MODEL_11a0992da8bb486e8eccf83f0c40b0e9",
            "value": "openai_humaneval/test-00000-of-00001.par(…): 100%"
          }
        },
        "6e3fc0fed26549a4ad12b0c02b1d45f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75e00f44589a41808059229caf0f13e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c8844be43b747fdae53bcb17089ee88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c75c7524f434a369c6ae5bf30803a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e6ed60d9d4547ef9e979724baaecf39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f37bebbf3443a8a4f5f3412b4fb657": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e3fc0fed26549a4ad12b0c02b1d45f3",
            "max": 83920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3291b98275e442b89b2a7ff5edaf8bcd",
            "value": 83920
          }
        },
        "9ae238fa23fc453789d65c6d3d4f2921": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3c73718d1224be1b9a0e773088598fc",
            "placeholder": "​",
            "style": "IPY_MODEL_36e0bbab8fd94e3f8a757b6c087ec3b2",
            "value": " 164/164 [00:00&lt;00:00, 1993.61 examples/s]"
          }
        },
        "9dc13497cecc4be89e055448000447da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a056f9488b224b77884d7810422404ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3e5a7281c4c48d091b30b7d7d3d62aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6581dccf386421dae004cd21dbdc7e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1de4f2537eb04b38a38ad1df730b9969",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b35f5c56f204173a0d3e6af3b9e9f36",
            "value": 1
          }
        },
        "d163ccbd5e494086bad2cbe544a9dd98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3c73718d1224be1b9a0e773088598fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec0ad7c545eb409ebfe4ac70fa80bd8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
