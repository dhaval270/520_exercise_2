{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b17ed622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/251.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m251.5/251.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q pytest pytest-cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0911fee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515d0071875e47baab6f0d78f04f4b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your token when prompted\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "KgSLtopoN-Ew",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c0d7b81a4049b89b512e509157bda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f2229cdce14557b935f7a25a82f5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openai_humaneval/test-00000-of-00001.par(‚Ä¶):   0%|          | 0.00/83.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0903cd1bbbd449eb9735a8f11b436fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['task_id', 'prompt', 'canonical_solution', 'test', 'entry_point']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"openai/openai_humaneval\", split=\"test\")\n",
    "print(dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "B0Dkt3noOEBI",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "       task_id                                             prompt  \\\n",
      "0  HumanEval/0  from typing import List\\n\\n\\ndef has_close_ele...   \n",
      "1  HumanEval/1  from typing import List\\n\\n\\ndef separate_pare...   \n",
      "2  HumanEval/2  \\n\\ndef truncate_number(number: float) -> floa...   \n",
      "3  HumanEval/3  from typing import List\\n\\n\\ndef below_zero(op...   \n",
      "4  HumanEval/4  from typing import List\\n\\n\\ndef mean_absolute...   \n",
      "\n",
      "                                  canonical_solution  \\\n",
      "0      for idx, elem in enumerate(numbers):\\n    ...   \n",
      "1      result = []\\n    current_string = []\\n    ...   \n",
      "2                              return number % 1.0\\n   \n",
      "3      balance = 0\\n\\n    for op in operations:\\n...   \n",
      "4      mean = sum(numbers) / len(numbers)\\n    re...   \n",
      "\n",
      "                                                test              entry_point  \n",
      "0  \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...       has_close_elements  \n",
      "1  \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...    separate_paren_groups  \n",
      "2  \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...          truncate_number  \n",
      "3  \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...               below_zero  \n",
      "4  \\n\\nMETADATA = {\\n    'author': 'jt',\\n    'da...  mean_absolute_deviation  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "n_problems = 10  # choose how many problems you want to load\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "dataset = load_dataset(\"openai/openai_humaneval\", split=\"test\")[:n_problems]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_tests = pd.DataFrame({\n",
    "    'task_id': dataset['task_id'],\n",
    "    'prompt': dataset['prompt'],\n",
    "    'canonical_solution': dataset['canonical_solution'],\n",
    "    'test': dataset['test'],\n",
    "    'entry_point': dataset['entry_point']\n",
    "})\n",
    "\n",
    "print(df_tests.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e20f97e",
   "metadata": {
    "id": "3e20f97e"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75f2fb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 120 code snippets.\n",
      "<bound method NDFrame.head of                                         generated_code\n",
      "0    from typing import List\\n\\n\\ndef has_close_ele...\n",
      "1    from typing import List\\n\\ndef has_close_eleme...\n",
      "2    Below is a clean, production‚Äëready implementat...\n",
      "3    print(has_close_elements([1.0, 2.0, 3.0], 0.5)...\n",
      "4    def has_close_elements(numbers: List[float], t...\n",
      "..                                                 ...\n",
      "115  def is_prime(num: int) -> bool:\\n    \"\"\"\\n    ...\n",
      "116  def _is_prime(n: int) -> bool:\\n    \"\"\"\\n    D...\n",
      "117  def is_prime(num: int) -> bool:\\n    \"\"\"\\n    ...\n",
      "118  def _is_prime(n: int) -> bool:\\n    \"\"\"\\n    R...\n",
      "119  def is_prime(num: int) -> bool:\\n    \"\"\"\\n    ...\n",
      "\n",
      "[120 rows x 1 columns]>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load your CSVs\n",
    "    df_code = pd.read_csv('/content/part1_results.csv', usecols=['generated_code'])\n",
    "    print(f\"Loaded {len(df_code)} code snippets.\")\n",
    "    print(df_code.head)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find .csv.\")\n",
    "    print(\"Please make sure your files are in the correct location.\")\n",
    "except ValueError as e:\n",
    "    print(f\"ERROR: A column might be missing. Make sure you have a 'code' and 'tests' column.\")\n",
    "    print(f\"Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0286b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated test files in 'generated_tests' (tests outer loop, code inner loop).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def _decode_text(s):\n",
    "    \"\"\"Decode escaped sequences if s is a string, else return as-is.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    try:\n",
    "        return s.encode(\"utf-8\").decode(\"unicode_escape\")\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "def _normalize_tests_for_injection(tests_cell):\n",
    "    \"\"\"\n",
    "    Return a list of test lines (strings) to insert inside the test function.\n",
    "    Handles:\n",
    "      - actual Python lists of assertions\n",
    "      - string like \"[assert ... , assert ...]\"\n",
    "      - multiline strings with '\\n' escapes\n",
    "    \"\"\"\n",
    "    if isinstance(tests_cell, list):\n",
    "        # assume already list of assertion strings\n",
    "        return [ _decode_text(str(x)).strip() for x in tests_cell if str(x).strip() ]\n",
    "\n",
    "    if not isinstance(tests_cell, str):\n",
    "        return []\n",
    "\n",
    "    s = _decode_text(tests_cell).strip()\n",
    "\n",
    "    # strip surrounding brackets if present\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        s = s[1:-1].strip()\n",
    "\n",
    "    # if multiple assertions separated by ', assert' -> split nicely\n",
    "    # also handle actual newlines\n",
    "    if \"\\n\" in s:\n",
    "        lines = [line.strip() for line in s.splitlines() if line.strip()]\n",
    "    elif \", assert\" in s:\n",
    "        # keep the leading \"assert\" on the first element\n",
    "        parts = s.split(\", assert\")\n",
    "        lines = []\n",
    "        for idx, p in enumerate(parts):\n",
    "            p = p.strip()\n",
    "            if idx > 0 and not p.startswith(\"assert\"):\n",
    "                p = \"assert \" + p\n",
    "            lines.append(p)\n",
    "    elif \"assert\" in s:\n",
    "        # single-line with one assert (or semicolon-separated)\n",
    "        if \";\" in s:\n",
    "            parts = [p.strip() for p in s.split(\";\") if p.strip()]\n",
    "            lines = [p if p.startswith(\"assert\") else (\"assert \" + p) for p in parts]\n",
    "        else:\n",
    "            lines = [s]\n",
    "    else:\n",
    "        # no 'assert' found; treat whole thing as single line\n",
    "        lines = [s] if s else []\n",
    "\n",
    "    # final clean-up: ensure each line is a valid expression string\n",
    "    return [line for line in lines if line]\n",
    "\n",
    "def generate_test_files(df_code, df_tests, output_dir=\"generated_tests\"):\n",
    "    \"\"\"\n",
    "    For each row in df_tests (index t_idx), create up to 5 files corresponding to\n",
    "    code rows at indices (t_idx*5 + iter_idx) where iter_idx in 0..4.\n",
    "    Each file contains the code snippet (decoded from escaped form) followed by\n",
    "    a test function `test_generated_snippet()` containing the test assertions.\n",
    "    Files are named: problem_{t_idx}_iteration_{iter_idx}.py\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    n_code = len(df_code)\n",
    "    n_tests = len(df_tests)\n",
    "\n",
    "    for t_idx, tests_row in df_tests.iterrows():\n",
    "        raw_tests = tests_row.get(\"tests\", \"\")\n",
    "        test_lines = _normalize_tests_for_injection(raw_tests)\n",
    "\n",
    "        for iter_idx in range(5):\n",
    "            code_idx = t_idx * 5 + iter_idx\n",
    "            if code_idx >= n_code:\n",
    "                # no corresponding code row ‚Äî skip\n",
    "                continue\n",
    "\n",
    "            code_cell = df_code.iloc[code_idx].get(\"code\", \"\")\n",
    "            if not isinstance(code_cell, str) or not code_cell.strip():\n",
    "                # skip empty code cells\n",
    "                continue\n",
    "\n",
    "            # decode escaped newlines/tabs etc.\n",
    "            code_text = _decode_text(code_cell).rstrip()\n",
    "\n",
    "            # ensure code ends with exactly one newline\n",
    "            if not code_text.endswith(\"\\n\"):\n",
    "                code_text = code_text + \"\\n\"\n",
    "\n",
    "            # build the test function body\n",
    "            file_lines = []\n",
    "            file_lines.append(code_text)\n",
    "            file_lines.append(\"def test_generated_snippet():\\n\")\n",
    "\n",
    "            if test_lines:\n",
    "                for line in test_lines:\n",
    "                    # indent each test line with 4 spaces\n",
    "                    file_lines.append(f\"    {line.rstrip()}\\n\")\n",
    "            else:\n",
    "                file_lines.append(\"    pass\\n\")\n",
    "\n",
    "            # final content\n",
    "            file_content = \"\".join(file_lines)\n",
    "\n",
    "            # write file\n",
    "            filename = os.path.join(output_dir, f\"problem_{t_idx}_iteration_{iter_idx}.py\")\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(file_content)\n",
    "\n",
    "    print(f\"‚úÖ Generated test files in '{output_dir}' (tests outer loop, code inner loop).\")\n",
    "\n",
    "generate_test_files(df_code, df_tests)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# generate_test_files(df_code, df_tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab36d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 120 code files into './generated_code'\n",
      "üöÄ Starting test run...\n",
      "Running tests in: /content/generated_tests\n",
      "Measuring coverage for: /content/generated_code\n",
      "======================================================================\n",
      "======================================================================\n",
      "‚úÖ All tests finished.\n",
      "üìä Generating final combined coverage report...\n",
      "======================================================================\n",
      "\n",
      "üåê Full HTML report generated: /content/htmlcov/index.html\n",
      "======================================================================\n",
      "üéâ Script finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "# V V V V V V V V V V V V V V V V V V V V V V V V V V V\n",
    "# --- EDIT THESE VARIABLES ---\n",
    "#\n",
    "# Point to your two new folders\n",
    "test_directory = \"/content/generated_tests\"\n",
    "csv_file_path = \"/content/part1_results.csv\"\n",
    "code_directory = \"./generated_code\"\n",
    "#\n",
    "# ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "\n",
    "def prepare_code_from_csv(csv_path: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Reads the CSV file and saves each code snippet from the 'generated_code' column\n",
    "    as a separate Python file in the output directory.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if \"generated_code\" not in df.columns:\n",
    "        raise ValueError(\"CSV file must contain a column named 'generated_code'\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for i, code in enumerate(df[\"generated_code\"], start=1):\n",
    "        file_path = Path(output_dir) / f\"code_{i}.py\"\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(code))\n",
    "    print(f\"‚úÖ Extracted {len(df)} code files into '{output_dir}'\")\n",
    "\n",
    "\n",
    "def run_all_tests(test_dir_str: str, src_dir_str: str):\n",
    "    \"\"\"\n",
    "    Runs pytest-cov on all tests and generates coverage reports.\n",
    "    \"\"\"\n",
    "    test_dir = Path(test_dir_str)\n",
    "    src_dir = Path(src_dir_str)\n",
    "\n",
    "    if not test_dir.is_dir():\n",
    "        print(f\"‚ùå Error: Test folder not found: {test_dir.resolve()}\")\n",
    "        return\n",
    "    if not src_dir.is_dir():\n",
    "        print(f\"‚ùå Error: Code folder not found: {src_dir.resolve()}\")\n",
    "        return\n",
    "\n",
    "    print(\"üöÄ Starting test run...\")\n",
    "    print(f\"Running tests in: {test_dir.resolve()}\")\n",
    "    print(f\"Measuring coverage for: {src_dir.resolve()}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Clear old coverage data\n",
    "    subprocess.run(\"coverage erase\", shell=True, capture_output=True)\n",
    "\n",
    "    # Run pytest with coverage\n",
    "    command = [\n",
    "        sys.executable,\n",
    "        \"-m\", \"pytest\",\n",
    "        f\"--cov={src_dir}\",\n",
    "        \"-v\",\n",
    "        \"--cov-branch\",\n",
    "        test_dir\n",
    "    ]\n",
    "\n",
    "    subprocess.run(command, text=True)\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚úÖ All tests finished.\")\n",
    "    print(\"üìä Generating final combined coverage report...\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    subprocess.run([sys.executable, \"-m\", \"coverage\", \"report\", \"-m\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"coverage\", \"html\"])\n",
    "\n",
    "    report_path = Path.cwd() / 'htmlcov' / 'index.html'\n",
    "    print(f\"\\nüåê Full HTML report generated: {report_path}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üéâ Script finished.\")\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "# Step 1: Prepare generated_code/ folder from CSV\n",
    "prepare_code_from_csv(csv_file_path, code_directory)\n",
    "\n",
    "# Step 2: Run all tests\n",
    "run_all_tests(test_directory, code_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3a4aaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "!! WARNING: LIVE RUN !!\n",
      "This script is about to permanently modify your files.\n",
      "PLEASE MAKE A BACKUP of your folder first.\n",
      "Target folder: ./generated_code\n",
      "You have 5 seconds to cancel (Ctrl+C)...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Script Finished.\n",
      "Total files scanned: 0\n",
      "Files ACTUALLY modified: 0\n",
      "Total 'test_' functions ACTUALLY removed: 0\n",
      "Operation complete.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "# V V V V V V V V V V V V V V V V V V V V V V V V V V V\n",
    "# --- 1. SET YOUR FOLDER PATH ---\n",
    "#\n",
    "# Point this to the folder containing your 'problem_*.py' files.\n",
    "target_directory = \"./generated_code\"  # <--- SET THIS\n",
    "\n",
    "# --- 2. SET 'DRY_RUN' ---\n",
    "#\n",
    "# True = Print what changes WOULD be made, without touching any files.\n",
    "# False = Actually modify and save the files.\n",
    "#\n",
    "# !! ALWAYS RUN WITH 'True' FIRST TO CHECK THE OUTPUT !!\n",
    "#\n",
    "DRY_RUN = False\n",
    "#\n",
    "# ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "class TestFunctionRemover(ast.NodeTransformer):\n",
    "    \"\"\"\n",
    "    This class visits every node in a Python file's AST.\n",
    "    If it finds a function definition (FunctionDef) whose name\n",
    "    starts with 'test_', it returns 'None', which effectively\n",
    "    removes it from the tree.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.functions_removed_count = 0\n",
    "        super().__init__()\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        if node.name.startswith('test_'):\n",
    "            self.functions_removed_count += 1\n",
    "            return None  # Remove this node\n",
    "        return node  # Keep this node\n",
    "\n",
    "\n",
    "def clean_test_functions(target_dir_str: str, dry_run: bool):\n",
    "    \"\"\"\n",
    "    Main function to process files.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- !! SAFETY WARNING !! ---\n",
    "    if not dry_run:\n",
    "        print(\"=\" * 70)\n",
    "        print(\"!! WARNING: LIVE RUN !!\")\n",
    "        print(\"This script is about to permanently modify your files.\")\n",
    "        print(\"PLEASE MAKE A BACKUP of your folder first.\")\n",
    "        print(f\"Target folder: {target_dir_str}\")\n",
    "        print(\"You have 5 seconds to cancel (Ctrl+C)...\")\n",
    "        print(\"=\" * 70)\n",
    "        try:\n",
    "            import time\n",
    "            time.sleep(5)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nOperation cancelled by user.\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"=\" * 70)\n",
    "        print(\"DRY RUN MODE - No files will be modified.\")\n",
    "        print(\"=\" * 70)\n",
    "    # --- End Warning ---\n",
    "\n",
    "    target_dir = Path(target_dir_str)\n",
    "\n",
    "    if not target_dir.is_dir():\n",
    "        print(f\"Error: Folder not found: {target_dir.resolve()}\")\n",
    "        return\n",
    "\n",
    "    files_processed = 0\n",
    "    files_to_modify = 0\n",
    "    total_funcs_removed = 0\n",
    "\n",
    "    # Use rglob to find 'problem_*.py' files in all subdirectories\n",
    "    for file_path in target_dir.rglob('problem_*.py'):\n",
    "        files_processed += 1\n",
    "        print(f\"\\n--- Processing: {file_path.relative_to(target_dir)} ---\")\n",
    "\n",
    "        try:\n",
    "            # Read the original file content\n",
    "            content = file_path.read_text()\n",
    "            if not content.strip():\n",
    "                print(\"File is empty. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Parse the content into an Abstract Syntax Tree\n",
    "            tree = ast.parse(content)\n",
    "\n",
    "            # Create our remover and have it \"visit\" the tree\n",
    "            remover = TestFunctionRemover()\n",
    "            new_tree = remover.visit(tree)\n",
    "\n",
    "            if remover.functions_removed_count > 0:\n",
    "                files_to_modify += 1\n",
    "                total_funcs_removed += remover.functions_removed_count\n",
    "\n",
    "                # Turn the modified AST back into Python source code\n",
    "                # Note: ast.unparse() adds newlines at the end\n",
    "                new_content = ast.unparse(new_tree)\n",
    "\n",
    "                if dry_run:\n",
    "                    print(f\"DRY RUN: Would remove {remover.functions_removed_count} 'test_' function(s).\")\n",
    "                else:\n",
    "                    # Write the modified content back to the file\n",
    "                    file_path.write_text(new_content)\n",
    "                    print(f\"SUCCESS: Removed {remover.functions_removed_count} 'test_' function(s) and saved file.\")\n",
    "            else:\n",
    "                print(\"No 'test_' functions found. Skipping.\")\n",
    "\n",
    "        except SyntaxError as e:\n",
    "            print(f\"SYNTAX ERROR: Could not parse file. Skipping.\")\n",
    "            print(f\"  Error details: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"UNEXPECTED ERROR during processing: {e}. Skipping file.\")\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Script Finished.\")\n",
    "    print(f\"Total files scanned: {files_processed}\")\n",
    "    if dry_run:\n",
    "        print(f\"Files that WOULD be modified: {files_to_modify}\")\n",
    "        print(f\"Total 'test_' functions that WOULD be removed: {total_funcs_removed}\")\n",
    "        print(\"\\nTo apply these changes, set DRY_RUN = False and re-run the cell.\")\n",
    "    else:\n",
    "        print(f\"Files ACTUALLY modified: {files_to_modify}\")\n",
    "        print(f\"Total 'test_' functions ACTUALLY removed: {total_funcs_removed}\")\n",
    "        print(\"Operation complete.\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# --- Run the function ---\n",
    "clean_test_functions(target_directory, DRY_RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_vezeUvxWs0q",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
